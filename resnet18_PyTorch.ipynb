{"cells": [{"metadata": {}, "cell_type": "markdown", "source": ""}, {"metadata": {}, "cell_type": "markdown", "source": ""}, {"metadata": {}, "cell_type": "markdown", "source": "<h1><h1>Pre-trained-Models with PyTorch </h1>\n"}, {"metadata": {}, "cell_type": "markdown", "source": "In this lab, you will use pre-trained models to classify between the negative and positive samples; you will be provided with the dataset object. The particular pre-trained model will be resnet18; you will have three questions: \n\n<ul>\n<li>change the output layer</li>\n<li> train the model</li> \n<li>  identify  several  misclassified samples</li> \n </ul>\nYou will take several screenshots of your work and share your notebook. \n"}, {"metadata": {}, "cell_type": "markdown", "source": "<h2>Table of Contents</h2>\n"}, {"metadata": {}, "cell_type": "markdown", "source": "\n"}, {"metadata": {}, "cell_type": "markdown", "source": "<h2 id=\"download_data\">Download Data</h2>\n"}, {"metadata": {}, "cell_type": "markdown", "source": "Download the dataset and unzip the files in your data directory, unlike the other labs, all the data will be deleted after you close  the lab, this may take some time:\n"}, {"metadata": {}, "cell_type": "code", "source": "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip ", "execution_count": 2, "outputs": [{"output_type": "stream", "text": "--2021-05-26 05:47:28--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip\nResolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\nConnecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2598656062 (2.4G) [application/zip]\nSaving to: \u2018Positive_tensors.zip\u2019\n\nPositive_tensors.zi 100%[===================>]   2.42G  36.6MB/s    in 70s     \n\n2021-05-26 05:48:38 (35.4 MB/s) - \u2018Positive_tensors.zip\u2019 saved [2598656062/2598656062]\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "!unzip -q Positive_tensors.zip ", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "! wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n!unzip -q Negative_tensors.zip", "execution_count": 4, "outputs": [{"output_type": "stream", "text": "--2021-05-26 05:51:57--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\nResolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\nConnecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2111408108 (2.0G) [application/zip]\nSaving to: \u2018Negative_tensors.zip\u2019\n\nNegative_tensors.zi 100%[===================>]   1.97G  36.3MB/s    in 60s     \n\n2021-05-26 05:52:57 (33.7 MB/s) - \u2018Negative_tensors.zip\u2019 saved [2111408108/2111408108]\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "We will install torchvision:\n"}, {"metadata": {}, "cell_type": "code", "source": "!pip install torchvision", "execution_count": 5, "outputs": [{"output_type": "stream", "text": "/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\n/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\nCollecting torchvision\n  Downloading torchvision-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (17.4 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 17.4 MB 13.9 MB/s eta 0:00:01\n\u001b[?25hCollecting torch==1.8.1\n  Downloading torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 804.1 MB 1.8 kB/s  eta 0:00:01MB/s eta 0:00:09:00:07\n\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from torchvision) (7.2.0)\nRequirement already satisfied: numpy in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from torchvision) (1.18.5)\nRequirement already satisfied: typing-extensions in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from torch==1.8.1->torchvision) (3.7.4.2)\nInstalling collected packages: torch, torchvision\n  Attempting uninstall: torch\n    Found existing installation: torch 1.3.1\n    Uninstalling torch-1.3.1:\n      Successfully uninstalled torch-1.3.1\nSuccessfully installed torch-1.8.1 torchvision-0.9.1\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>\n"}, {"metadata": {}, "cell_type": "markdown", "source": "The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it.\n"}, {"metadata": {}, "cell_type": "code", "source": "# These are the libraries will be used for this lab.\nimport torchvision.models as models\nfrom PIL import Image\nimport pandas\nfrom torchvision import transforms\nimport torch.nn as nn\nimport time\nimport torch \nimport matplotlib.pylab as plt\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport h5py\nimport os\nimport glob\ntorch.manual_seed(0)", "execution_count": 100, "outputs": [{"output_type": "execute_result", "execution_count": 100, "data": {"text/plain": "<torch._C.Generator at 0x7fda8c19d350>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "from matplotlib.pyplot import imshow\nimport matplotlib.pylab as plt\nfrom PIL import Image\nimport pandas as pd\nimport os", "execution_count": 101, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<!--Empty Space for separating topics-->\n"}, {"metadata": {}, "cell_type": "markdown", "source": "<h2 id=\"data_class\">Dataset Class</h2>\n"}, {"metadata": {}, "cell_type": "markdown", "source": " This dataset class is essentially the same dataset you build in the previous section, but to speed things up, we are going to use tensors instead of jpeg images. Therefor for each iteration, you will skip the reshape step, conversion step to tensors and normalization step.\n"}, {"metadata": {}, "cell_type": "code", "source": "# Create your own dataset object\n\nclass Dataset(Dataset):\n\n    # Constructor\n    def __init__(self,transform=None,train=True):\n        directory=\"\"\n        positive=\"Positive_tensors\"\n        negative='Negative_tensors'\n\n        positive_file_path=os.path.join(directory,positive)\n        negative_file_path=os.path.join(directory,negative)\n        positive_files=[os.path.join(positive_file_path,file) for file in os.listdir(positive_file_path) if file.endswith(\".pt\")]\n        negative_files=[os.path.join(negative_file_path,file) for file in os.listdir(negative_file_path) if file.endswith(\".pt\")]\n        number_of_samples=len(positive_files)+len(negative_files)\n        self.all_files=[None]*number_of_samples\n        self.all_files[::2]=positive_files\n        self.all_files[1::2]=negative_files \n        # The transform is goint to be used on image\n        self.transform = transform\n        #torch.LongTensor\n        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n        self.Y[::2]=1\n        self.Y[1::2]=0\n        \n        if train:\n            self.all_files=self.all_files[0:30000]\n            self.Y=self.Y[0:30000]\n            self.len=len(self.all_files)\n        else:\n            self.all_files=self.all_files[30000:]\n            self.Y=self.Y[30000:]\n            self.len=len(self.all_files)     \n       \n    # Get the length\n    def __len__(self):\n        return self.len\n    \n    # Getter\n    def __getitem__(self, idx):\n               \n        image=torch.load(self.all_files[idx])\n        y=self.Y[idx]\n                  \n        # If there is any transform method, apply it onto the image\n        if self.transform:\n            image = self.transform(image)\n\n        return image, y\n    \nprint(\"done\")", "execution_count": 102, "outputs": [{"output_type": "stream", "text": "done\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "We create two dataset objects, one for the training data and one for the validation data.\n"}, {"metadata": {}, "cell_type": "code", "source": "train_dataset = Dataset(train=True)\nvalidation_dataset = Dataset(train=False)\nprint(\"done\")", "execution_count": 103, "outputs": [{"output_type": "stream", "text": "done\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "<h2 id=\"Question_1\">Question 1</h2>\n"}, {"metadata": {}, "cell_type": "markdown", "source": "<b>Prepare a pre-trained resnet18 model :</b>\n"}, {"metadata": {}, "cell_type": "markdown", "source": "<b>Step 1</b>: Load the pre-trained model <code>resnet18</code> Set the parameter <code>pretrained</code> to true:\n"}, {"metadata": {}, "cell_type": "code", "source": "# Step 1: Load the pre-trained model resnet18\n\n# Type your code here\nmodel=models.resnet18(pretrained=True)", "execution_count": 104, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<b>Step 2</b>: Set the attribute <code>requires_grad</code> to <code>False</code>. As a result, the parameters will not be affected by training.\n"}, {"metadata": {}, "cell_type": "code", "source": "# Step 2: Set the parameter cannot be trained for the pre-trained model\nfor param in model.parameters():\n    param.requires_grad=False\n\n# Type your code here", "execution_count": 105, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<code>resnet18</code> is used to classify 1000 different objects; as a result, the last layer has 1000 outputs.  The 512 inputs come from the fact that the previously hidden layer has 512 outputs. \n"}, {"metadata": {}, "cell_type": "markdown", "source": "<b>Step 3</b>: Replace the output layer <code>model.fc</code> of the neural network with a <code>nn.Linear</code> object, to classify 2 different classes. For the parameters <code>in_features </code> remember the last hidden layer has 512 neurons.\n"}, {"metadata": {}, "cell_type": "code", "source": "model.fc=nn.Linear(512,2)", "execution_count": 106, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Print out the model in order to show whether you get the correct answer.<br> <b>(Your peer reviewer is going to mark based on what you print here.)</b>\n"}, {"metadata": {}, "cell_type": "code", "source": "print(model)", "execution_count": 107, "outputs": [{"output_type": "stream", "text": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=2, bias=True)\n)\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Train the Model\n"}, {"metadata": {}, "cell_type": "markdown", "source": "\n"}, {"metadata": {}, "cell_type": "markdown", "source": "<b>Step 1</b>: Create a cross entropy criterion function \n"}, {"metadata": {}, "cell_type": "code", "source": "# Step 1: Create the loss function\n\n# Type your code here\ncriterion=nn.CrossEntropyLoss()", "execution_count": 108, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<b>Step 2</b>: Create a training loader and validation loader object, the batch size should have 100 samples each.\n"}, {"metadata": {}, "cell_type": "code", "source": "train_loader=DataLoader(dataset=train_dataset,batch_size=100)\nvalidation_loader=DataLoader(dataset=validation_dataset,batch_size=100)", "execution_count": 109, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<b>Step 3</b>: Use the following optimizer to minimize the loss \n"}, {"metadata": {}, "cell_type": "code", "source": "optimizer = torch.optim.Adam([parameters  for parameters in model.parameters() if parameters.requires_grad],lr=0.001)", "execution_count": 110, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "train_dataset[0][0].shape", "execution_count": 111, "outputs": [{"output_type": "execute_result", "execution_count": 111, "data": {"text/plain": "torch.Size([3, 224, 224])"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "len(train_dataset)/100", "execution_count": 19, "outputs": [{"output_type": "execute_result", "execution_count": 19, "data": {"text/plain": "300.0"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "<!--Empty Space for separating topics-->\n"}, {"metadata": {}, "cell_type": "markdown", "source": " calculating  the accuracy on the validation data for one epoch.\n"}, {"metadata": {}, "cell_type": "code", "source": "n_epochs=1\nloss_list=[]\naccuracy_list=[]\ncorrect=0\nN_test=len(validation_dataset)\nN_train=len(train_dataset)\nstart_time = time.time()\n#n_epochs\ni=0\nj=0\nLoss=0\nstart_time = time.time()\nfor epoch in range(n_epochs):\n    for x, y in train_loader:\n        i+=1\n\n        model.train() \n        #clear gradient \n        optimizer.zero_grad()\n     \n        #make a prediction \n        yhat=model(x)\n   \n        # calculate loss \n        loss=criterion(yhat,y)\n    \n        # calculate gradients of parameters \n        loss.backward()\n        \n        # update parameters \n        optimizer.step()\n        \n        loss_list.append(loss.data)\n        print(i,loss.data)\n    correct=0\n    for x_test, y_test in validation_loader:\n        j+=1\n        # set model to eval \n        model.eval()\n       \n        #make a prediction \n        z=model(x_test)\n        \n        #find max\n        _,yhat=torch.max(z.data,1)\n       \n       \n        #Calculate misclassified  samples in mini-batch \n        correct+=(yhat==y_test).sum().item()\n        print(j)\n        \n   \n    accuracy=correct/N_test\n    accuracy_list.append(accuracy)\n\n", "execution_count": 112, "outputs": [{"output_type": "stream", "text": "1 tensor(0.7238)\n2 tensor(0.6569)\n3 tensor(0.6293)\n4 tensor(0.5898)\n5 tensor(0.5623)\n6 tensor(0.4928)\n7 tensor(0.4898)\n8 tensor(0.4632)\n9 tensor(0.4263)\n10 tensor(0.4072)\n11 tensor(0.3466)\n12 tensor(0.3616)\n13 tensor(0.3294)\n14 tensor(0.3591)\n15 tensor(0.3386)\n16 tensor(0.2773)\n17 tensor(0.2492)\n18 tensor(0.2913)\n19 tensor(0.2702)\n20 tensor(0.2198)\n21 tensor(0.2264)\n22 tensor(0.2867)\n23 tensor(0.2239)\n24 tensor(0.1915)\n25 tensor(0.1804)\n26 tensor(0.1625)\n27 tensor(0.1782)\n28 tensor(0.1658)\n29 tensor(0.1450)\n30 tensor(0.1467)\n31 tensor(0.1409)\n32 tensor(0.1643)\n33 tensor(0.1503)\n34 tensor(0.1710)\n35 tensor(0.1315)\n36 tensor(0.1204)\n37 tensor(0.1268)\n38 tensor(0.1409)\n39 tensor(0.1198)\n40 tensor(0.1413)\n41 tensor(0.1330)\n42 tensor(0.1128)\n43 tensor(0.1004)\n44 tensor(0.1075)\n45 tensor(0.1092)\n46 tensor(0.1075)\n47 tensor(0.1302)\n48 tensor(0.0929)\n49 tensor(0.0901)\n50 tensor(0.0957)\n51 tensor(0.0853)\n52 tensor(0.0902)\n53 tensor(0.0673)\n54 tensor(0.1134)\n55 tensor(0.0732)\n56 tensor(0.0985)\n57 tensor(0.1050)\n58 tensor(0.0904)\n59 tensor(0.0776)\n60 tensor(0.0708)\n61 tensor(0.0838)\n62 tensor(0.0810)\n63 tensor(0.1038)\n64 tensor(0.0916)\n65 tensor(0.0902)\n66 tensor(0.0917)\n67 tensor(0.0721)\n68 tensor(0.0686)\n69 tensor(0.0679)\n70 tensor(0.0731)\n71 tensor(0.1001)\n72 tensor(0.0748)\n73 tensor(0.0867)\n74 tensor(0.0541)\n75 tensor(0.1056)\n76 tensor(0.0756)\n77 tensor(0.0637)\n78 tensor(0.0829)\n79 tensor(0.0619)\n80 tensor(0.0602)\n81 tensor(0.0599)\n82 tensor(0.0702)\n83 tensor(0.0672)\n84 tensor(0.0740)\n85 tensor(0.0554)\n86 tensor(0.0595)\n87 tensor(0.0790)\n88 tensor(0.0454)\n89 tensor(0.0549)\n90 tensor(0.0503)\n91 tensor(0.0589)\n92 tensor(0.0905)\n93 tensor(0.0446)\n94 tensor(0.0679)\n95 tensor(0.0497)\n96 tensor(0.0535)\n97 tensor(0.0622)\n98 tensor(0.0688)\n99 tensor(0.0696)\n100 tensor(0.0856)\n101 tensor(0.0495)\n102 tensor(0.0857)\n103 tensor(0.0494)\n104 tensor(0.0418)\n105 tensor(0.0603)\n106 tensor(0.0632)\n107 tensor(0.0850)\n108 tensor(0.0554)\n109 tensor(0.0443)\n110 tensor(0.0603)\n111 tensor(0.0509)\n112 tensor(0.0564)\n113 tensor(0.0454)\n114 tensor(0.0683)\n115 tensor(0.0517)\n116 tensor(0.0416)\n117 tensor(0.0494)\n118 tensor(0.0390)\n119 tensor(0.0443)\n120 tensor(0.0530)\n121 tensor(0.0648)\n122 tensor(0.0421)\n123 tensor(0.0830)\n124 tensor(0.0556)\n125 tensor(0.0343)\n126 tensor(0.0300)\n127 tensor(0.0297)\n128 tensor(0.0315)\n129 tensor(0.0448)\n130 tensor(0.0566)\n131 tensor(0.0504)\n132 tensor(0.0683)\n133 tensor(0.0332)\n134 tensor(0.0354)\n135 tensor(0.0512)\n136 tensor(0.0460)\n137 tensor(0.0376)\n138 tensor(0.0306)\n139 tensor(0.0646)\n140 tensor(0.0643)\n141 tensor(0.0513)\n142 tensor(0.0340)\n143 tensor(0.0396)\n144 tensor(0.0422)\n145 tensor(0.0315)\n146 tensor(0.0453)\n147 tensor(0.0461)\n148 tensor(0.0413)\n149 tensor(0.0554)\n150 tensor(0.0352)\n151 tensor(0.0630)\n152 tensor(0.0601)\n153 tensor(0.0525)\n154 tensor(0.0429)\n155 tensor(0.0292)\n156 tensor(0.0300)\n157 tensor(0.0383)\n158 tensor(0.0395)\n159 tensor(0.0498)\n160 tensor(0.0432)\n161 tensor(0.0503)\n162 tensor(0.0461)\n163 tensor(0.0400)\n164 tensor(0.0335)\n165 tensor(0.0465)\n166 tensor(0.0398)\n167 tensor(0.0391)\n168 tensor(0.0429)\n169 tensor(0.0368)\n170 tensor(0.0255)\n171 tensor(0.0247)\n172 tensor(0.0337)\n173 tensor(0.0302)\n174 tensor(0.0277)\n175 tensor(0.0245)\n176 tensor(0.0181)\n177 tensor(0.0731)\n178 tensor(0.0293)\n179 tensor(0.0373)\n180 tensor(0.0355)\n181 tensor(0.0418)\n182 tensor(0.0632)\n183 tensor(0.0349)\n184 tensor(0.0509)\n185 tensor(0.0493)\n186 tensor(0.0252)\n187 tensor(0.0218)\n188 tensor(0.0298)\n189 tensor(0.0387)\n190 tensor(0.0629)\n191 tensor(0.0365)\n192 tensor(0.0248)\n193 tensor(0.0487)\n194 tensor(0.0347)\n195 tensor(0.0279)\n196 tensor(0.0481)\n197 tensor(0.0402)\n198 tensor(0.0211)\n199 tensor(0.0294)\n200 tensor(0.0452)\n201 tensor(0.0688)\n202 tensor(0.0485)\n203 tensor(0.0465)\n204 tensor(0.0236)\n205 tensor(0.0253)\n206 tensor(0.0268)\n207 tensor(0.0684)\n208 tensor(0.0534)\n209 tensor(0.0348)\n210 tensor(0.0395)\n211 tensor(0.0322)\n212 tensor(0.0367)\n213 tensor(0.0421)\n214 tensor(0.0582)\n215 tensor(0.0289)\n216 tensor(0.0650)\n217 tensor(0.0409)\n218 tensor(0.0376)\n219 tensor(0.0297)\n220 tensor(0.0215)\n221 tensor(0.0291)\n222 tensor(0.0347)\n223 tensor(0.0456)\n224 tensor(0.0183)\n225 tensor(0.0231)\n226 tensor(0.0501)\n227 tensor(0.0463)\n228 tensor(0.0263)\n229 tensor(0.0296)\n230 tensor(0.0227)\n231 tensor(0.0349)\n232 tensor(0.0379)\n233 tensor(0.0658)\n234 tensor(0.0255)\n235 tensor(0.0605)\n236 tensor(0.0523)\n237 tensor(0.0568)\n238 tensor(0.0354)\n239 tensor(0.0272)\n240 tensor(0.0288)\n241 tensor(0.0267)\n242 tensor(0.0446)\n243 tensor(0.0498)\n244 tensor(0.0252)\n245 tensor(0.0702)\n246 tensor(0.0292)\n247 tensor(0.0378)\n248 tensor(0.0284)\n249 tensor(0.0261)\n250 tensor(0.0481)\n251 tensor(0.0319)\n252 tensor(0.0307)\n253 tensor(0.0255)\n254 tensor(0.0177)\n255 tensor(0.0311)\n256 tensor(0.0243)\n257 tensor(0.0227)\n258 tensor(0.0262)\n259 tensor(0.0640)\n260 tensor(0.0310)\n261 tensor(0.0239)\n262 tensor(0.0242)\n263 tensor(0.0242)\n264 tensor(0.0148)\n265 tensor(0.0274)\n266 tensor(0.0251)\n267 tensor(0.0123)\n268 tensor(0.0340)\n269 tensor(0.0152)\n270 tensor(0.0416)\n271 tensor(0.0285)\n272 tensor(0.0220)\n273 tensor(0.0232)\n274 tensor(0.0173)\n275 tensor(0.0247)\n276 tensor(0.0429)\n277 tensor(0.0193)\n278 tensor(0.0281)\n279 tensor(0.0347)\n280 tensor(0.0441)\n281 tensor(0.0199)\n282 tensor(0.0226)\n283 tensor(0.0205)\n284 tensor(0.0191)\n285 tensor(0.0177)\n286 tensor(0.0278)\n287 tensor(0.0257)\n288 tensor(0.0359)\n289 tensor(0.0492)\n290 tensor(0.0279)\n291 tensor(0.0502)\n292 tensor(0.0373)\n293 tensor(0.0227)\n294 tensor(0.0243)\n295 tensor(0.0194)\n296 tensor(0.0224)\n297 tensor(0.0173)\n298 tensor(0.0157)\n299 tensor(0.0107)\n300 tensor(0.0307)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "<b>the Accuracy and plot the loss stored in the list <code>loss_list</code> for every iteration and take a screen shot.</b>\n"}, {"metadata": {}, "cell_type": "code", "source": "accuracy", "execution_count": 113, "outputs": [{"output_type": "execute_result", "execution_count": 113, "data": {"text/plain": "0.9944"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "plt.plot(loss_list)\nplt.xlabel(\"iteration\")\nplt.ylabel(\"loss\")\nplt.show()\n", "execution_count": 114, "outputs": [{"output_type": "display_data", "data": {"text/plain": "<Figure size 432x288 with 1 Axes>", "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXhU5dn48e+dTPaVrBBI2BcDsokodbdK0epLtVp3azdrq63dq29bu/ettXur8qPW1rZW6i5VFGsVkEUW2bdAEiAJScieTNbJJM/vj3NmMglJCJjDJMz9ua5czJxzZuY+OeTc8+xijEEppVToCgt2AEoppYJLE4FSSoU4TQRKKRXiNBEopVSI00SglFIhzhXsAE5WWlqaGTduXLDDUEqpYeX999+vMsak97Zv2CWCcePGsWXLlmCHoZRSw4qIHOlrn1YNKaVUiNNEoJRSIU4TgVJKhThNBEopFeI0ESilVIjTRKCUUiFOE4FSSoW4kEkEeeVufrkyj5omT7BDUUqpISVkEsGhqkb++E4+xxpagx2KUkoNKSGTCOKjIgBobPMGORKllBpaQicRRFuzaTS2aiJQSqlAoZMIoqxE4NYSgVJKdeNoIhCRRSKSJyL5IvJAL/u/KSLb7Z/dItIhIilOxJJglwjcre1OvL1SSg1bjiUCEQkHHgWuAnKBW0QkN/AYY8wjxpjZxpjZwIPAamNMjRPx+EoEWjWklFLdOVkimA/kG2MKjTEeYBmwuJ/jbwGecSqY2MhwRLSxWCmlenIyEYwGigOel9jbjiMiscAi4IU+9t8tIltEZEtlZeUpBSMixEe5cGuJQCmlunEyEUgv20wfx14LrOurWsgYs9QYM88YMy89vdcFdgYkIcqlJQKllOrByURQAmQHPB8DlPZx7M04WC3kEx/t0jYCpZTqwclEsBmYLCLjRSQS62a/vOdBIpIEXAK84mAsgNVgrCUCpZTqzrE1i40xXhG5D1gJhANPGmP2iMg99v4l9qHXAW8aY5qcisUnPjqC+hbtPqqUUoEcXbzeGLMCWNFj25Iez/8K/NXJOHwSolwcrW0+HR+llFLDRsiMLAatGlJKqd6EViLQxmKllDpOaCWCKBdNng46OvvqxaqUUqEnpBKBb76hJo+WCpRSyiekEoHON6SUUscLrURglwgadAZSpZTyC6lEMDIxGoCyel2uUimlfEIqEeSkxAJQXKNjCZRSyiekEkF6QhRRrjCKqjURKKWUT0glAhEhJyWWIi0RKKWUX0glAoCxqZoIlFIqUMglguyUWIprmjFGB5UppRSEYCLISYmlydNBTZMn2KEopdSQEHKJYMwIu+dQbUuQI1FKqaEh5BJBZmIUAJXutiBHopRSQ0PIJYL0BE0ESikVKOQSQWqcJgKllAoUcokg0hVGcmwEVY2aCJRSChxOBCKySETyRCRfRB7o45hLRWS7iOwRkdVOxuOTHh+lJQKllLI5tmaxiIQDjwJXAiXAZhFZbozZG3BMMvAYsMgYUyQiGU7FEyg9IYpKLREopRTgbIlgPpBvjCk0xniAZcDiHsfcCrxojCkCMMZUOBiPX3qClgiUUsrHyUQwGigOeF5ibws0BRghIqtE5H0RudPBePx8VUM6ulgppRysGgKkl20977wu4Bzgw0AMsEFE3jPGHOj2RiJ3A3cD5OTkfODA0hOiaGnvoMnT4V+1TCmlQpWTJYISIDvg+RigtJdj3jDGNBljqoA1wKyeb2SMWWqMmWeMmZeenv6BA0uL1y6kSinl42Qi2AxMFpHxIhIJ3Aws73HMK8BFIuISkVjgPGCfgzEBkGmvVFauK5UppZRzicAY4wXuA1Zi3dyfNcbsEZF7ROQe+5h9wBvATmAT8IQxZrdTMflkp8QAUFyr01ErpZSjFeTGmBXAih7blvR4/gjwiJNx9JSVHEOY6JKVSikFITiyGCAiPIys5BhdoEYppQjRRADokpVKKWUL6USgVUNKKRXKiSA1lqpGD01t3mCHopRSQRW6iSDFWqlMq4eUUqEuZBNBRoI1lqC6UdcuVkqFtpBNBHFR4QA0trUHORKllAqukE0ECVERADS2dQQ5EqWUCq6QTQTx0dZYusZWLREopUJbyCYCX9VQk0dLBEqp0BayiSDKFU5keBjuVu0+qpQKbSGbCMAqFeg4AqVUqAvpRBAf7aJRE4FSKsSFdCKIi9REoJRSIZ0IEqJdNGobgVIqxIV0IoiLctHk0USglAptIZ0I4qO0RKCUUpoItI1AKRXiNBFoIlBKhThHE4GILBKRPBHJF5EHetl/qYjUi8h2++chJ+PpKS7KRbOng45Oczo/VimlhhTHFq8XkXDgUeBKoATYLCLLjTF7exz6rjHmGqfi6E+CPd9Qk8dLYnREMEJQSqmgc7JEMB/IN8YUGmM8wDJgsYOfd9LiouxEoNVDSqkQ5mQiGA0UBzwvsbf1tEBEdojI6yIyvbc3EpG7RWSLiGyprKwctADjo3wzkGoiUEqFLicTgfSyrWdl/FZgrDFmFvAH4OXe3sgYs9QYM88YMy89PX3QAvQlggZNBEqpEOZkIigBsgOejwFKAw8wxjQYYxrtxyuACBFJczCmbiakx+EKE779wk7qm3VdAqVUaHIyEWwGJovIeBGJBG4GlgceICIjRUTsx/PteKodjKmbsalxPHrbXPIrGllzcPCqnJRSajhxrNeQMcYrIvcBK4Fw4EljzB4RucfevwS4AfiCiHiBFuBmY8xp7ct50WSrAFJU03w6P1YppYYMxxIB+Kt7VvTYtiTg8R+BPzoZw4nERrpIT4jiSHVTMMNQSqmgCemRxT5jU2I5Uq0lAqVUaNJEgNVWoIlAKRWqNBEAY1NjKW9opbVdF7JXSoUeTQRYiQCgWBuMlVIhSBMBkJNiJQLtOaSUCkWaCICRSdEAHGtoC3IkSil1+mkiANLioxCBYw2twQ5FKaVOO00EQER4GKlxkVS4tUSglAo9mghs6QnRVLq1RKCUCj2aCGyZiVHaRqCUCkmaCGwZCVFUaIlAKRWCNBHYMhOjqXS36frFSqmQo4nAlpEQRaeB6iatHlJKhRZNBLaMRGssQYW2EyilQowmAlumnQjK6rWdQCkVWjQR2CakxwGQV94Q5EiUUur00kRgS4yOYGxqLHtKNREopUKLJoIA07MSNREopUKOJoIA07OSKKpppr6lPdihKKXUaeNoIhCRRSKSJyL5IvJAP8edKyIdInKDk/GcSG5WIgB7tVSglAohjiUCEQkHHgWuAnKBW0Qkt4/jHgZWOhXLQI1LtRqMyxtaghyJUkqdPk6WCOYD+caYQmOMB1gGLO7luC8BLwAVDsYyIAnRLgAaWrxBjkQppU4fJxPBaKA44HmJvc1PREYD1wFL+nsjEblbRLaIyJbKyspBD9THlwjcrdpGoJQKHU4mAullW8+JfH4LfNsY0++q8caYpcaYecaYeenp6YMWYE9RrnCiXGE0tGqJQCkVOlwOvncJkB3wfAxQ2uOYecAyEQFIA64WEa8x5mUH4+pXYkyElgiUUiFlQCUCEblfRBLF8mcR2SoiC0/wss3AZBEZLyKRwM3A8sADjDHjjTHjjDHjgOeBLwYzCYBVPaRtBEqpUDLQqqFPG2MagIVAOvAp4Of9vcAY4wXuw+oNtA941hizR0TuEZF7PkDMjkqMjqBBSwRKqRAy0KohX33/1cBfjDE7xK7P6Y8xZgWwose2XhuGjTF3DTAWRyVEu7SNQCkVUgZaInhfRN7ESgQrRSQB6HQurOBJjInArSOLlVIhZKAlgs8As4FCY0yziKRgVQ+dcRK1RKCUCjEDLREsAPKMMXUicjvwXaDeubCCR9sIlFKhZqCJ4HGgWURmAd8CjgB/cyyqIEqIduHxdtLUpqUCpVRoGGgi8BpjDNYUEb8zxvwOSHAurOBJjIkAYPr3V/LuwUrWHKikxdPveDellBrWBpoI3CLyIHAH8Jo9UVyEc2EFj2+aCYD/7D3GnU9u4plNRUGMSCmlnDXQRHAT0IY1nqAca86gRxyLKogSo7vy24aCagD2lum01EqpM9eAEoF9838aSBKRa4BWY8wZ2kbQlQgOVjQCsF/XMVZKncEGOsXEJ4BNwI3AJ4CNwV5ExilRruN/JQePNeLtOCOHTSil1IDHEXwHONcYUwEgIunAW1jzA51RZo5J4vvX5rK9uI5Xtltz5LV5Ozlc3cykjPggR6eUUoNvoG0EYb4kYKs+idcOKyLCpy4Yz5RMq1OUr/F4n7YTKKXOUAO9mb8hIitF5C4RuQt4jR5zCJ1pspKjAZg/LgWA8vrWYIajlFKOGWhj8TeBpcBMYBaw1BjzbScDC7ZRSTEATBmZgIiuWqaUOnMNeGEaY8wLWGsLh4RxqXGECUzOiCc+SucfUkqdufotEYiIW0Qaevlxi8gZXWk+Mima1++/mP+ZlaXzDymlzmj9lgiMMWfkNBIDNXVkV4OxW0sESqkz1BnZ82ewJUZH0KBrFCilzlCaCAZASwRKqTOZJoIBSIyJwN2mJQKl1JnJ0UQgIotEJE9E8kXkgV72LxaRnSKyXUS2iMiFTsZzqrREoJQ6kw24++jJsqeqfhS4EigBNovIcmPM3oDD/gssN8YYEZkJPAtMcyqmU5UYHYG71YsxBhEJdjhKKTWonCwRzAfyjTGFxhgPsAxrYRs/Y0yjveANQBxgGIISol10dBqadYEapdQZyMlEMBooDnheYm/rRkSuE5H9WNNWfLq3NxKRu+2qoy2VlZWOBNsf39TUWj2klDoTOZkIeqtDOe4bvzHmJWPMNOBjwI97eyNjzFJjzDxjzLz09PRBDvPEEmOsGrSG1naWrimgsLLxtMeglFJOcTIRlADZAc/HAKV9HWyMWQNMFJE0B2M6Jb4SwdHaFn62Yj//3Ni1dOWBY27qdYyBUmoYczIRbAYmi8h4EYkEbgaWBx4gIpPEbn0VkblAJNYU10OKbyrqwqomAPKOuf37blyygT+tKQxKXEopNRgc6zVkjPGKyH3ASiAceNIYs0dE7rH3LwE+DtwpIu1AC3BTQOPxkOFbx/hQlVUllFduJYI2bwf1Le1UutuCFptSSn1QjiUCAGPMCnqsW2AnAN/jh4GHnYxhMCTaJYJDdomgwt1GbZOHDjtn6WAzpdRwpiOLB2BEXCRhAvvKuqqE8o65/fMPaW8ipdRwpolgACLCwxiVFENNk8e/La/c7U8AulaBUmo400QwQNkp1oplKXGRRIaHUVbf6l+jQFcvU0oNZ5oIBih7RCwAafGRJMVGUN/ioaHFKglo1ZBSajhztLH4TJKTYiWC1LgoAGqb2v0lAS0RKKWGM00EA5TtSwTxkXR0GmqbPf6qodb2Tto7OokI1wKWUmr40TvXAPnaCNLio0iOjaC+pb1blZBWDymlhitNBAPkKxGkxUcyIjbSKhEETC2h1UNKqeFKE8EAZSRE88gNM7lxXjbJsRHUNrd36zaqJQKl1HClbQQn4cZ51hx6ybGReLydVLhb/fs0ESilhistEZyCEbHW3ENFNc3+Cem0akgpNVxpIjgFyXYiKK5pYXSy1YisJQKl1HClieAUJMdG+h9n+ROBlgiUUsOTJoJTMKJbIogGtESglBq+NBGcAl/VEMCMrCSiI8L8g8uUUmq40URwCgITwcfPGUN6QhQVujiNUmqY0u6jpyDKFc6XLp/EZdMy/FNUl9W1nviFSik1BGkiOEVfXzjV/zgrKZotR2qDGI1SSp06R6uGRGSRiOSJSL6IPNDL/ttEZKf9s15EZjkZj1NGJcdwrKGVzs4ht9yyUkqdkGOJQETCgUeBq4Bc4BYRye1x2CHgEmPMTODHwFKn4nFSVlI07R2GqkZtJ1BKDT9OlgjmA/nGmEJjjAdYBiwOPMAYs94Y46tTeQ8Y42A8jhmVZI0lKK3XdgKl1PDjZCIYDRQHPC+xt/XlM8Drve0QkbtFZIuIbKmsrBzEEAfHKHssQVldS5AjUUqpk+dkIpBetvVaiS4il2Elgm/3tt8Ys9QYM88YMy89PX0QQxwcvmkmjmoiUEoNQ072GioBsgOejwFKex4kIjOBJ4CrjDHVDsbjmKSYCGIiwimp1USglBp+nCwRbAYmi8h4EYkEbgaWBx4gIjnAi8AdxpgDDsbiKBFh3rgRrMqrwBjtOaSUGl4cSwTGGC9wH7AS2Ac8a4zZIyL3iMg99mEPAanAYyKyXUS2OBWP064+exSHq5vZW9YQ7FCUUuqkODqgzBizAljRY9uSgMefBT7rZAyny8LcTL7z0i7e2F1OalwUsVHhJEZHnPiFSikVZDqyeJCkxkcxY3QSWw7X8sr2UmZlJ/OHW+YEOyyllDohTQSDaOaYJP61uZj2DkNNkwePt5NIV/fat0p3GwZDRkJ0kKJUSqnudPbRQTRzdDLtHVZjcWObl82Ha4475qv/2s7Xn91xukNTSqk+aSIYRDOzkwCIj3IR6Qrjv/sq/Pv2lTVw8JibHSV1FNU0BytEpZQ6jlYNDaJJ6fHERIRzztgRtHg62FlS59/3jed2UN3owd3qpUMnp1NKDSGaCAaRKzyMR26cSU5KLM9tKeHlbUcxxtBpIL+ikTZvJwDNng6a2rzERemvXykVfHonGmTXzMwCYGdJPe42L0frWjAGfxLwqWps00SglBoS9E7kkLNGJQCQV+4mLOz4aZcq3W2MTY073WEppdRxNBE4ZEqmlQgeemUPuVmJAFw7K4vaJg9r86uo1DWOlVJDhCYChyRER5CZGMXRuhb/rKR/uGUOFe5W5v/0v70uYmOMQaS3SVuVUso5mggc9ORd53Koqon7/rnNvy01LoowwV8i+P1/D1Jc00xSTASv7Chl44Mf7rUqSSmlnKKJwEHTs5KYnpVEZHgYSTHWvEPhYUJKXBSVjW0YY/jX5mJqmjy0tHcAsOpABZdPywxm2EqpEKMDyk6DhdNHct6EVP/z9IQoKt1tFNU0c7SuxZ8EAP6y7nAQIlRKhTJNBEGQlRRNfkUj6/K7r8MTH+ViY2EN7R3du5r+e0cpn1iyQdc6UEo5QhNBEFw2LYPD1c38Zd0hUuMi/ds/NicLT0cnh6qauh3/zv4KNh2uoaHFe7pDVUqFAE0EQbBoxkjCBA5WNPKpC8aRkxILwP/MGg1Y8xIFOljRCECFu/X0BqqUCgmaCIIgLT6KCyalkZ0Sw2cvmsBZoxIYnRzD7OxkIsKFfWVu/7GdnYZ8fyLQsQdKqcGnvYaC5I+3zMXb2Ul0RDgPXTud+uZ2Il1hTMpIYH95V4mgtL6rMflYw/ElAm9HJwWVTUwdmXDaYldKnVm0RBAkSbERpMZHATA6OcY/+visUQnsPlpPZ6fB29HJzpJ6/2t6KxG8tquMq363ptckoZRSA+FoIhCRRSKSJyL5IvJAL/unicgGEWkTkW84GctwccHENKoaPTyxtpBLHlnFF5/eCoArTKhoOD4RHK1rodPgH708WFo8HXTqdNlKhQTHEoGIhAOPAlcBucAtIpLb47Aa4MvAL52KY7i5dGo6IvCzFfsJDxPuOH8sN83LJjsllmO9NBbXNHoAqHK3sS6/iot+8TYVPUoHzR4vP351L3XNngHF0NFpuOgXb/P0pqIPfkJKqSHPyRLBfCDfGFNojPEAy4DFgQcYYyqMMZuBdgfjGFZS46OYk50MwPevzeXHH5vBwzfMtAah9VIiqLFv7uUNrXzvld0U17Sw6kBlt2PW51fz57WHeGHr0V4/0xjDktUFlNqlioaWdqoaPewOqJZSSp25nEwEo4HigOcl9raTJiJ3i8gWEdlSWVl54hcMc5+/ZCJ3LhjL5dMy/NsyE6OpcLdS39LOuwe7fge1TVYieGZTMYWVTYSHCe8VdB+oll9p9Tp6e/+xXj+vqKaZn7++n+ffLwGgrsXKy4Nd3aSUGpqcTAS9zZx2SpXOxpilxph5xph56enpHzCsoe8j00fyo8Uzus1EmpEQRXlDK19+Zht3/HkThfbNvcZOBPvKGhCBy6am815hdbdRyL7upxsLa6hqbGNrUW23+v8j1dYayoftgWy+KqSSWl1bWalQ4GQiKAGyA56PAUod/Lwz2kWT0/B4O1ltV/t89+Xd3Lx0A+UB7QFZSTFcMiWd0vpW/80drESQFh+Jt9Mw7ydvcf1j63l9d7l//5Ea69hD1b5EYJUISutaP1CD8S9X5vHgi7tO+fVKqdPDyUSwGZgsIuNFJBK4GVju4Oed0S6dmsGyuxfw9SunsGBCKusLqnmvsIZjAe0G49JiuXCyVWJaY1cftXd0UlDRyFUzRvHPz53Hpy8YD8COkjr/64rsBFBQ0cgNj6/n1Z1lAHg6OqnsZd2EgVpzsJL1BVV97t9f3sBzW4rZX97Ak2sPddvX2Ob1Jz2llLMcSwTGGC9wH7AS2Ac8a4zZIyL3iMg9ACIyUkRKgK8B3xWREhFJdCqm4W7++BS+9OHJ3H3xBFy9rFkwLjWO8WlxjEuN5Z39FewqqWfyd17H3eZlUkY8H5qYxkPX5nL26CT2lnYNWiuySwQNrV62HKnlha0l/n0ltS3cv2wbr+8qO+l4S+ta/FVXvVn023f55vM7eWZjET96dS+tAbOw/uO9I3zyyU2nvJJbU5uXTyzZ0G1wnlKqd46OIzDGrDDGTDHGTDTG/NTetsQYs8R+XG6MGWOMSTTGJNuP9S/3BC6blsHeHy0i3E4GsZHhAIxPs9ZAvnRqBhsKq3lxW9cNfU5Osv9x7qhE9pU1+NsRjlQ3E+nq/b/CjuI6Xtleyr939l+r9+TaQzy3patvQGt7B1WNHtytXjzezn5eCfvKrSk1Am/6++35lopPsZ0iv6KRTYdrePdA3yUSpZRFRxYPU5GuMMbak9VNttdHHpdqJYIrzsqktb2Tf7x3hPMnpLDj+wuZOaYrEZw1KoHqJg8VbmtxnKKaZs4bn9Lt/aPsxLByj9WWsL/cTU97Sxv8DcqPry5g2eauRFBe39V2UdvL+IXAHkl7jlrdVANHTucdsxq4S2pPreeSbylQX2nnTNPUpjPRqsGjiWAYm5gRD8CsMUkATLETwgWTUpmbk0x7h+GSKRn+1dF8crOs49/eX8H24jqaPR1cOjWDiyanMSopGoCRSdFMTI9j46EawOpR5Ku6OVzVxIFjbq7+/bvc/sRGKtytVLrbut38SwNu9L1VD205XON/3OSx3tc3EK6j01BQ6UsEp3YjP5MTQVl9C7N/9CabDtWc+OAQcuCYm/oWHZJ0KjQRDGMT061EcNt5Y3n1SxeSk2qVEESE716Ty+jkGK6aMfK4183OTmZOTjLffXk3n/rrZrKSorlh7hj+/pnzuG6ONdQjOSaCK3O7XttprOqWNm8H1z22joW/WQPA4epmf3vDsYZWOuxeRqUBSeH590t4euMR//Mlqwv4yr+2HxeXb76kI9VN/uqk3koER+ta+NzftlDf3PcffZU94rq/RGCM4YEXdrIuf3hVHxXXtNDeYThYcXwpbbD8bMU+fvzq3uO2v/B+Sa/bndDm7TjxQTZjDDc8vp4lqwscjOjMpYlgGLt0ajpTMuPJSYllxuikbvvm5oxg3QOXM85uNwgU6Qrj7585j7s+NI6pmQn8/pY5JMVapQZfiSApNpIrc621kyfZJY+8cjfv7K+ktrmdnJRYf/XRqjyrd4+301BtfxMPLBH8ee0hfv76fowxFNc08+v/HOCiyek8f8+CbnEds6uGDhyzbnBRrrBeE8Hb+yv4z95jrO3lBl7T5OH2Jzb6Sxwltc3+5NRTWX0ryzYX80ZAV9pAv3hjPzf9vw2sGGBDeVVjG/kO3px9fN96qxu7l7TWF1TR7Om7yqiwspGG1oF9Y35jdzmv9tIu9PXndvDntYdo7Kdqas2BSu76y6Y+f+8DUVrXwtk/GHipp8nTQUOrl6OnUJW49mCVvwo0VGkiGMbOn5DKm1+9hBi7sfhkxEe5+N41ufzr8wuYN66rfWBUUgxglQjmZCczf3wKd188geiIMB5fXcAf3j5IWnwkb3/9Ev726fkA3RqJy+ySQGldC5HhXf+93K1ejlQ384e3DyLAwx8/m3njUrqt0HasoZWCykZ+sHwvCdEuFkxMpaS2mUp3G4+tysdrL+FZYA+Q21pUe9x5vb67jLX5VbyT5+s+a7qNtQi00+5C29f+v793hI2Havj9fw/285vs8tAru7ni12t4av3hAR1/qnyJILDK7WhdC7f+aSPPbi7u9TWdnYbrHlvPHwZwLq3tHRTXNnOsoa3PXl9bjxz/u/d592Alq/Iqu30ZOFn7yxvweDvZW1rPz1/ff8L38s25Vd108r3M/vjOQR5+Y/8pxXmm0ESguhmVbJUIkmMjCAsTnv38Aj4xL5vf3zwHsBqNbztvLK7wMKbbpZAmT4e/1FBS24Ixhn3lbqaNSiBgcDSr8ip4eXspN84b4084GYnW5/lmV/3nxiJqmj0su/t8JmfEc7S2hWWbivjFG3m8uPUoT60/TJ7dcN1bInhzT9c0Gr5eVUequy/9aYzhqfWHeWW79Y23tym8W9s7cLd6SYh2sb/c3a39o6eG1naqG9v8cf341b2Ojsr2JYKqxuN7WR2u7v1zyxqs6Ul8q93153B1E76B6T273/oS98ZD1T1fxrObi1n02zWU22NbfO08p6LIPo+1+dUsWV3Axx5dx2/fOtDntCe+BFDlHtjEioEq7PatUF4TXBOB6iYroEQQaOH0kbz1tUvI/+lVfPXKKYBVqsiyq5Ieu20uAPf+cysLf7OGHcV1LJ49ultD9Q/+vRePt5M7F4zzb8tMtNZkmJKZQIW7lV1H68kdlcj0rCRyUuNo83byyg7rhv3tF3fy/eV72FBo3YT2HG3oVo/sbm3vNoAtd5Q1JOVwVfebY0FlI99fvsc/utqXCDo7jf9m4Nt287nW4Pg1/Qxu+9ZzO/nkXzZR3eThirMyEIH/t7qwz+M/qHq7F1bgt/U8uzqtuI82Ed+UJEf6SBSBCiq6EmfgannGGJrsqqeNhcdX2by+u4z95W72ltbbn9l03DEDVVRj3fB971XhbuO3bx3kmY29z4jr+11UncIAyMqGNpo9HSG9JrgmAtXNiLhIHroml+vnjmSPTO4AABhBSURBVOl1f+D8RwAv3XsBOx5ayGS7RADWGsuxkeHcOG8MKbGR3Y7/6MxR/t5NAJkJViI5e3QSZfWt7C1t4Gy7pHHpFGuUdH5FI5HhYQR+YZuVnYyno5O1B6t4bFU+zR4vy3eU0t5hmGav1jZzTBIxEeH+NgefN/d2n3yv0t2Gt6OThb9dwzee2wl0VXFdPCWdzMQo/0jtnto7Onn3YCV7Shuoa25n/vgUrp8zhufeLz6uPn5HcR0/f30/6wuquPfprQOeFryn3qqGDtilkYMVjVz+q1Xc8eeN3Uo6vptycU0z3o5O9pY2+Htp1Te3d2tb8CWNpJiIbgMPmz0dtLZb1XO7jtb7q+rASqJbi6yqtgL7swqr+i8RlNa18MWn3z+uxAZdjfylPUpiPceVdHQafrkyz5+oa5o93eKqbfL0227S4unAbbd3lDWc3kkWOzsNT288Qk2Th3uf3npa2pf6oolAHefTF47vtZG5N5mJ0STFRnRLEN+7JpeHPz6TxOgIUuIiiQwP4ycfm8H1c0bzqxtndXv9ZdPS+ejMUczMTsLd6qWxzetPBNkpsf7H31o0lR8tns7i2VlWjBeMIy4ynK/8azu/eCOPF7Ye5e8bjpA7KpGP20ksIyGaKZnxxyeCPcc4e3QSX71iCredl0OngfeP1JJf0cgLW0t4c0+5vypoVFIM88alsK2ortt75Fc08sN/7+Hbz++kydPhT1LZI2K59bwcWts7eXVH90bm3/33IEtWF/C5p7bw2q4yPve3LXg7OjlwzM1Xlm0b8NiArqqhwBKBddMtqmmmsLKJdw9WdWurOGRPKOjtNJTVt/LJv2zih/+2ev9c+Iu3ufLXa/zHFlQ2Mjo5hgUTUnlxWwmPvpNvf571bfuiyWm0eTu7VTMVVDYe13WztxKBu7Wdf7x3BGMMaw9WsWJXOZc8suq4HmA9q9b2/ugjXDY13V/95rNkdQF/fCefpzZYvdKM6ZqaHeCmpRv4wfI9x8XhUxGwxkdZP9V/p+JEAynXFVTxnZd287MV+3htVxnPbOq9fed00ESgBs1VM0ayYEIqn7lwPNfOsm7YOSmx5GYlcvv5Y/n1TbOJjujesL1oxigevXUu18/pKoEE9oC6ZuYoAK7MzeTOBeO4dX4OybERnDc+lUUzRuFutW6ev3vrAPvL3dy5YCyTMq3SSVpCJFNHJnRLBJXuNrYX17EwN5P7r5jMpVOtqb598ysB/Po/B/w3hZFJ0czJTuZoXQuV7jZe2X6U9flV3L9sG39df5gXt3Vf4yE7JZaZY5KYmpnAswGN6DVNHv+31iZPB9fNGc3mw7X8c1MR/9xYxMvbS/nTu4VsLarly89s63dqDt8Nt7bZ41/StKCikfioriXIU+Iiu92ICyob/W0m24rrqHS3sc7uZeRu9XK0roXd9sC+fWVuJmXE8/ANM5k/LoWn37Nusr5E4Pud+Y4H2NJL43FvieDrz+7guy/vZkdJfbdv9xsKu6r0fIMcfdLiI4mNdDFlZAKFlU2029/4mz1efvfW8Y3fvnaCpjYvB441si7/+PYMn8BBjGV1J04ENU2eXnsy5Ve4+fqzO/w3/5e2lTDlu6/3WVUHXdWNvh5Lq/IqTvj5TtFEoAbN47efwzN3n99t2w8XT+fJu8494WtjIsN58KppZCVFMzmzq5rpUxeM56Uvfoix9qjp8yaksv2hhYxMiubW87KJcoVxZW4mVY0epmclcv3cMcwek8xZoxKZmzOCKZkJVDV6/DextfnWH5/vZjbSbqx+bVcZidEuHv742ewvd/Pi1hISolzER7mYZS8UdOVvVnP/su3c9ueN7Clt4L7LJpGZGMX0rESiI6w/pZzUWESE/5mdxfbiOqoa2yipbeYbz+3A22n4461z+N+rp/HrT8ziQxNT+dWbB/iPXVX12KoC7nhiI8t3lPKLfnqx+BJBR6dhT2kDL28vxdPR6R8zMjE9jrk5I7pVzRRWNnHO2BEArNpv3XDqmtt5eVtXF9G/rDtMfUs7ByrczM0ZQVJMBBfbs9k2tnn9JZBzx40gLjKcHyzfw3de2kWzx8vSNYXkpMSSbHdDnp6VSHlDK8/0WOXOVy1XWtfCkepm0uKjCBPYG9AWUd3kodnT4f+dZtrXaGpmAp6OTn9V0t7SBjwdnSREu7p9xrsHK6lubPNPv360rqXPNb0Dl38trz9x1dCf3i3k5qUbjkvUL207ygtbS/xfOn6w3Cpt7Snte3GnNfb0J74vMwWVTf0mDidpIlCOSrCrhwbi85dMZP2DHyYioNtppCuMOTkjej3+nLEp7PnhR/jJx2Zw0eQ0fnfzbCJdYYyIi+T1+y/irFGJTLXbC3x/oGsOVJEaF8n0LKshOTPJaqyuafJwztgRLJ49mrT4SA5WNDLSbgifYY/ErmtuZ3RyjL8a6PJpGaz+5mU8+/kFTB2ZSFJMBInR1o3wwklpAKzLr+K7L+9mQ0E1n79kAtfMzOLuiyciInz/2uk0tlnfxu+9bCLXzR7NxVPS+fjcMSzbXOy/kZXVt3Tr0VLf0o5vzsFr/7iWbzy3g5ljkvjMRePt38sIJqbHcbi62U4W9Ryta2FhbiZRrjDeCfjm+cS7VqP2tJEJbCuqZWtRLcZYN3voGrRYUNHoT6bpCVHERLpo8nTw9MYibv3TRg5VNfHzj59N9ghrUOPXF07hkinpPPjiLv/vPj+gKqmoppkjNc1MHRnP+LQ49tm9ntq8HXzt2R2AtX43dCVrX9uSrwF7h72Cnq+60Fci+r/X9/Odl3Z3q7rqq7urr2ooJiL8uPaIQI1tXg5VNVFY2UinodviUAA77VgKKhspqW32J+u+enGV17eSd8xNRLh1IX1/I321RTlNE4Ea1lzhYWQmRvP3z5zHpIyE4/ZPz0oiIlx47J0CdpbUseZAJRdOTiPMvpOmxUX5j73p3ByiI8K555KJQFe9ekxkOBPT40iKiWDF/RcR5QojIdrFzDHJREeEExfl4sZzxnDTuV3Lb8wYnURSTAR/33CEVXmVfOHSiTx41VndYps6MoE7zh+LCHxiXjYP3zCTx28/h28vmooIvLazjLf2HmPB/73tH5AHUN/iZYx9wwWr2uxn153NuNQ4zh6dxDUzs5iQHofH28nR2hb+su6w1Xh/TjbnjkuhtrmdyPAwckclUljVRKQrjI9MH8mh6iZW51USHibMticp9HULzq9o9Fe5pMZFcc8lE5idnUx2Sgzbi+u497KJfGhiGmNGWL3OxqXG8ZubZhMdEcaf1ljJ5ldv5vknNyyqaaaouomclDjOsidB9HZ08tbeCtYcqOQH1+Zy9dlWtWCmnZAnZ8YzIjaC5XYvsp0ldYxMjOa88an+/T5v7TvG+oIqIsPDiHSF8Y+NR/wliU2HanjfTgyV7jZcYcKUkQn9dhH+w9sH+Z8/rPU3hAdOkW6MYdfRrp5Sj68q8M8OXNhHF1rfYL0bzrH+z1xid0rYUNB3NdYzm4rY5dDysa4TH6LU8JUSF8lPrzubbz2/k7V/rCLKFcZN87pu2GFhwsv3XkBKbKR/io7bzx/LT17bxw3ndLVbvHTvBbjChNhIF5+9aDxhIv46d99rAoWHCRdMSmXFrnJiIsK57bycXuP736vP4vq5o/1VX2CNrZg3dgSv7y5jy5EaXGHC/1tTyN6yBn5z02waWtqZMTqRoppmPjQxlT/dOc//2n9/6UIANtsjqzceqmb59lJuOjebpNgIFk7PZG1+FWNTY/nGR6bw6b9uwePt5OzRSRhjTSExPSuR2Ejr1jA2NRZXmJBf2Yi7tZ2kmAgiXWF89qIJfPaiCbxXWM26/Cq+eoXVpdiXCDISo4mPcvGJedk8s6mI2TnJvL67nG8vmsYbe8rZc7Se2uZ2xqbG0tFpeHVnGZO+8zpTMxMYERvBHQvG+af+8JUIolzh3HH+WP7wTj43LlnP5sO1XHFWJhPSrd/d2JRYf6O+t9Pw4tajTMqI5+oZI1myupDbntjIivsv4qv/2k5rewerv3UZFe420hOimJoZz2s7y6hubCM13vpycKS6iX1lDSzMHcnuo/W427y47VLGmgOVtHd0EhFujX73Lea0ck85BysaueP8sewtbTiunaS1vYM/rSnklR2lzByTxNVnj+SZTUVMzoy3GtDzrdUFe/bOa23v4Lsv7+aeSyZw9pjuswgMBk0E6oz3iXnZTEyPp6S2mQUTU8mwu6z6zM5O7vY8OiKcfT9a1G1qbl+VD8A3PzJtQJ/79YVTmZszgounpPtvLj1FusK6zQzrs2jGKH786l72l7v5yhWTSY2P4qev7WXRb9fg6ehk/vgUrpox0v+tuacJdq+vX715AE9HJ3ddMA6wSg8PvbKHCelxXDY1gxvOGcOsMUn+Bnp3m5drZ2b53yciPIzxaXH8470jNLZ5OadHNd35E1I5f0Kq//kt83PISYn1V9N89sIJ/OO9I3z35d2MT4vjcxeNZ29ZA/+2v9WPTYntNtYk75ib6+eMJjxMyLDHmPiq6ADuWDCOpzYc8bdXLJiY6p9+PSUuiqV3nMPkzASWrinkmU1FnDUqka8tnMql0zK4cckGvvCP9/2D0h56ZTcbC2vISo7h7osn8Pz7JXzlX9uZlBHP2aOTeGnbUd49WMUVZ2Vy4FjXN/tLpqSz+kAlj71TwOXTMjhSY93sMxKi2F/uJjoijC9dPolHVubx5t5j1DV7SLa7Ub+07Si/+s8BwOpdNys7mXPHjeDyaRmkxkXy8vZSPvvUFh68+izGpsb6q0nzKxrp6DScNcqZ5Vo0EaiQcM7YEf7G0oE4lWk7epqYHu+vYz9Zt87PoayuhcKqJm4/fyxp8VHMHJ3E4kfXAZAcE8lN5/ZeygBIjY9i8ewsXtleyqVT0/1xjEqK4cuXT2LO2BGICL+0u/MaY0iNi6TN28lN87O7vdeCiams2FXGpz40iTsCBgP2ZkJ6PBMCzjknNZarZozitV1lfPHSibjCu6ZPB6s6Z1JGAq9+6UL2lTXwzed3snC6NcfV1MwEfvKxGXw0INmlJ0Sx9XtXEh4mVDS0khIXiSs8jB9/bAbnjU/xtyP83/Vnc8M5Y8i2Syhzc0Zw+3k5/m6mV+Zm8uLWoyREufj9LXOYlGFV0z2zqZiNh2owxhBmfyt/a1/3cSd3XTCO9o5OfvPWAX7z1gHGp8WRnhDFBZPSeGnbUa6dmUVqfBQT0uOoafIw+0f/YdH0kXzmovG8tPUo49Pi+PSF47lh7hhiIsN57p4PARBnl8L+u7+C/eVu6po9fO7iCXzliin+NhSnEoEMt2HV8+bNM1u2bAl2GEoFxf3LtvHK9lIeuWEmN87L7vfYjk7DPzce4eIp6d2qnvryxLuFxEa6uLWXaqzeqisG6kh1E8s2F/O1K6cQER7mLyF89OxRPGqPSPd9xtaiOubmJJ/yZ/XnaF0Ll/ziHUaPiGH1Ny8jv6KRKFcY2QGJCayOBb7Zdb93Ta5/ttWIcKG9w/DONy4lIlx4Y3c5T649RGl9K9+7JpcwgR/+ey8vfvFDzM0ZwftHavn44+v58LQMNh6q8U/U942FU7jv8sm9xrj7qNWw/4V/vE9cpItGj5fvfTSXwqpGnn+/hD0/XNStSvJkiMj7xph5ve7TRKDU8OHrqvmpC8Yft87EcFHf0s5zW4q5/fyxx40rcdpT6w+THBvB4tmj+z3u+sfWsbesge0PLeTK36ymuKaFq2aM5L/7Ktj9w4/4qw3fK6zmH+8d4ZEbZhHpCqOoptlfVQXg7ejEFR5Gs8fLqzvLeK+gmv/96Fmk9VFV6FNc00xCtIvP/W0Lmw9bDduzspN55d4LTvncNREopdRJOHjMzdG6Fi6dmsFPX9vL8h2lvH7/xRyqauScsSknfoNBYozhsVUFPLIyj0unpvPXT80/5fcKWiIQkUXA74Bw4AljzM977Bd7/9VAM3CXMWZrf++piUApdTp5vJ00tXkZMcDxMIOts9Pw27cOcGXuyA/UY6i/ROBYY7GIhAOPAlcCJcBmEVlujAlc3ugqYLL9cx7wuP2vUkoNCZGuMCJdwUkCYHVx/trCqc5+hoPvPR/IN8YUGmM8wDJgcY9jFgN/M5b3gGQR6b0/nFJKKUc4mQhGA4HT6ZXY2072GETkbhHZIiJbKiuDMwRbKaXOVE4mgt76OPVskBjIMRhjlhpj5hlj5qWnpw9KcEoppSxOJoISILCj8xig52rYAzlGKaWUg5xMBJuBySIyXkQigZuB5T2OWQ7cKZbzgXpjTFnPN1JKKeUcx3oNGWO8InIfsBKr++iTxpg9InKPvX8JsAKr62g+VvfRTzkVj1JKqd45OteQMWYF1s0+cNuSgMcGuNfJGJRSSvVP1yNQSqkQN+ymmBCRSuDIKb48Dag64VHDg57L0KTnMjTpucBYY0yv3S6HXSL4IERkS19DrIcbPZehSc9laNJz6Z9WDSmlVIjTRKCUUiEu1BLB0mAHMIj0XIYmPZehSc+lHyHVRqCUUup4oVYiUEop1YMmAqWUCnEhkwhEZJGI5IlIvog8EOx4TpaIHBaRXSKyXUS22NtSROQ/InLQ/ndEsOPsjYg8KSIVIrI7YFufsYvIg/Z1yhORjwQn6t71cS4/EJGj9rXZLiJXB+wbkuciItki8o6I7BORPSJyv7192F2Xfs5lOF6XaBHZJCI77HP5ob3d2etijDnjf7DmOioAJgCRwA4gN9hxneQ5HAbSemz7BfCA/fgB4OFgx9lH7BcDc4HdJ4odyLWvTxQw3r5u4cE+hxOcyw+Ab/Ry7JA9F2AUMNd+nAAcsOMddteln3MZjtdFgHj7cQSwETjf6esSKiWCgayWNhwtBp6yHz8FfCyIsfTJGLMGqOmxua/YFwPLjDFtxphDWBMSnvqK3YOsj3Ppy5A9F2NMmbHXBzfGuIF9WItCDbvr0s+59GUon4sxxjTaTyPsH4PD1yVUEsGAVkIb4gzwpoi8LyJ329syjT1tt/1vRtCiO3l9xT5cr9V9IrLTrjryFduHxbmIyDhgDta3z2F9XXqcCwzD6yIi4SKyHagA/mOMcfy6hEoiGNBKaEPcBcaYucBVwL0icnGwA3LIcLxWjwMTgdlAGfAre/uQPxcRiQdeAL5ijGno79Betg31cxmW18UY02GMmY21UNd8EZnRz+GDci6hkgiG/UpoxphS+98K4CWs4t8xERkFYP9bEbwIT1pfsQ+7a2WMOWb/8XYCf6KraD6kz0VEIrBunE8bY160Nw/L69LbuQzX6+JjjKkDVgGLcPi6hEoiGMhqaUOWiMSJSILvMbAQ2I11Dp+0D/sk8EpwIjwlfcW+HLhZRKJEZDwwGdgUhPgGzPcHarsO69rAED4XERHgz8A+Y8yvA3YNu+vS17kM0+uSLiLJ9uMY4ApgP05fl2C3kp/G1virsXoTFADfCXY8Jxn7BKyeATuAPb74gVTgv8BB+9+UYMfaR/zPYBXN27G+wXymv9iB79jXKQ+4KtjxD+Bc/g7sAnbaf5ijhvq5ABdiVSHsBLbbP1cPx+vSz7kMx+syE9hmx7wbeMje7uh10SkmlFIqxIVK1ZBSSqk+aCJQSqkQp4lAKaVCnCYCpZQKcZoIlFIqxGkiUCFLRNbb/44TkVsH+b3/t7fPUmoo0u6jKuSJyKVYs1RecxKvCTfGdPSzv9EYEz8Y8SnlNC0RqJAlIr5ZHn8OXGTPWf9Ve9KvR0Rksz1h2eft4y+1573/J9ZAJUTkZXsiwD2+yQBF5OdAjP1+Twd+llgeEZHdYq0vcVPAe68SkedFZL+IPG2PmFXKca5gB6DUEPAAASUC+4Zeb4w5V0SigHUi8qZ97HxghrGm/AX4tDGmxp4OYLOIvGCMeUBE7jPWxGE9XY81CdosIM1+zRp73xxgOtZcMeuAC4C1g3+6SnWnJQKljrcQuNOeCngj1vD+yfa+TQFJAODLIrIDeA9r8q/J9O9C4BljTYZ2DFgNnBvw3iXGmiRtOzBuUM5GqRPQEoFSxxPgS8aYld02Wm0JTT2eXwEsMMY0i8gqIHoA792XtoDHHejfpzpNtESgFLixljj0WQl8wZ7aGBGZYs/62lMSUGsngWlYSwr6tPte38Ma4Ca7HSIda+nLITHzpQpd+o1DKWumR69dxfNX4HdY1TJb7QbbSnpfBvQN4B4R2Yk18+N7AfuWAjtFZKsx5raA7S8BC7BmkjXAt4wx5XYiUSootPuoUkqFOK0aUkqpEKeJQCmlQpwmAqWUCnGaCJRSKsRpIlBKqRCniUAppUKcJgKllApx/x/ciHmPR+moLQAAAABJRU5ErkJggg==\n"}, "metadata": {"needs_background": "light"}}]}, {"metadata": {}, "cell_type": "code", "source": "j=0\nyhat_list=[]\ny_list=[]\nyhatt=[]\nyy=[]\ncount=0\nfor x_test, y_test in validation_loader:\n        j+=1\n        # set model to eval \n        model.eval()\n       \n        #make a prediction \n        z=model(x_test)\n        #print(z.shape)\n        #find max\n        _,yhat=torch.max(z.data,1)\n        #print(yhat.shape)\n        #print(y.shape)\n        yhat_list=yhat.tolist()\n        y_list=y_test.tolist()\n        #print(y_list)\n        #print(yhat_list)\n\n        for i in range(0,100):\n            if yhat_list[i]!=y_list[i]:\n                print('Sample number:',(j-1)*100+i,'The predicted value is',yhat_list[i],'and the real value is',y_list[i])\n                count=count+1\n                #print(count)\n            if count==4:\n                break\n        if count==4:\n            break\n            \n       \n                \n            \n        #print(j)\n        ", "execution_count": 115, "outputs": [{"output_type": "stream", "text": "Sample number: 460 The predicted value is 0 and the real value is 1\nSample number: 484 The predicted value is 0 and the real value is 1\nSample number: 731 The predicted value is 1 and the real value is 0\nSample number: 828 The predicted value is 0 and the real value is 1\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Find the misclassified samples</h2> \n"}, {"metadata": {}, "cell_type": "markdown", "source": "<b>Identify the first four misclassified samples using the validation data:</b>\n"}, {"metadata": {}, "cell_type": "code", "source": "validation_loaderr=DataLoader(dataset=validation_dataset,batch_size=1)\ncount=0\nj=0\nfor x_test, y_test in validation_loaderr:\n        j+=1\n        # set model to eval \n        model.eval()\n       \n        #make a prediction \n        z=model(x_test)\n        \n        #find max\n        _,yhat=torch.max(z.data,1)\n\n        if yhat !=y_test:\n            count=count+1\n            #print(count)\n            print('sample number',j,'Real value:',y_test.item(),'predicted value:',yhat.item())\n        if count==4:\n            break\n\n\n", "execution_count": 116, "outputs": [{"output_type": "stream", "text": "sample number 461 Real value: 1 predicted value: 0\nsample number 485 Real value: 1 predicted value: 0\nsample number 732 Real value: 0 predicted value: 1\nsample number 829 Real value: 1 predicted value: 0\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "validation_loaderr=DataLoader(dataset=validation_dataset,batch_size=1)\ncount=0\nj=0\nfor x_test, y_test in validation_loaderr:\n        j+=1\n        # set model to eval \n        model.eval()\n       \n        #make a prediction \n        z=model(x_test)\n        \n        #find max\n        _,yhat=torch.max(z.data,1)\n\n        if yhat !=y_test:\n            count=count+1\n            #print(count)\n            print('sample number',j,'Actual value:',y_test,'predicted value:',yhat)\n        if count==4:\n            break\n\n\n", "execution_count": 99, "outputs": [{"output_type": "stream", "text": "sample number 461 Actual value: tensor([1]) predicted value: tensor([0])\nsample number 485 Actual value: tensor([1]) predicted value: tensor([0])\nsample number 732 Actual value: tensor([0]) predicted value: tensor([1])\nsample number 829 Actual value: tensor([1]) predicted value: tensor([0])\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": ""}, {"metadata": {}, "cell_type": "markdown", "source": "\n"}, {"metadata": {}, "cell_type": "markdown", "source": ""}, {"metadata": {}, "cell_type": "markdown", "source": "\n"}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.9", "language": "python"}, "language_info": {"name": "python", "version": "3.9.7", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}